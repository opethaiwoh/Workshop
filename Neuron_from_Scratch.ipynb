{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neuron from Scratch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opethaiwoh/Workshop/blob/main/Neuron_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Objectives\n",
        "- Train the network weights for the neuron.\n",
        "- Make predictions with the neuron.\n",
        "- Implement the neuron for a real-world classification problem on the [Sonar Dataset](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)).\n",
        "\n",
        "A single neuron can be used for two-class classification problems where a linear equation (like a hyperplane) can be used to separate the two classes. It is inspired by the way human neurons process information. The neuron receives its input signals from incoming connections (typically from other neurons). These input signals are combined in an equation called the **activation**.\n",
        "\n",
        "**activation = sigmoid(bias + sum(w_i * a_i))**\n",
        "\n",
        "where \n",
        "\n",
        "- the **sum** is taken over the inputs of the neuron\n",
        "- each input **a_i** has a weight **w_i** associated with it \n",
        "\n",
        "- **sigmoid(x) = 1 / (1 + e^(-x))** is the sigmoid function, which squeezes the numbers line into the (0,1) range.\n",
        "\n",
        "The activation of the neuron can then be transformed into an output value (prediction) using a **transfer function** such as the step transfer function.\n",
        "\n",
        "**prediction = 1.0 if activation >= 0.5 else 0.0**\n"
      ],
      "metadata": {
        "id": "dFEIJPrRyl5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Develop a function that can make predictions\n",
        "\n",
        "A good practice with the neuron function is to normalize the input data (all values between 0 and 1).\n",
        "\n",
        "The first weight is always the bias. It is not associated with any inout values. \n",
        "\n",
        "We can use previously prepared weights to make predictions for a dataset.\n",
        "\n",
        "Below is a function named predict() that predicts an output value for a row given a set of weights."
      ],
      "metadata": {
        "id": "7p5INrhb0ClV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Make a prediction with weights\n",
        "def predict(row, weights):\n",
        "\tactivation = weights[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tactivation += weights[i + 1] * row[i]\n",
        "\tactivation = 1/(1 + np.exp(-activation)) # Sigmoid Function\n",
        "\treturn 1.0 if activation >= 0.5 else 0.0\n",
        "\n",
        "# test predictions\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t         [1.465489372,2.362125076,0],\n",
        "\t         [3.396561688,4.400293529,0],\n",
        "\t         [1.38807019,1.850220317,0],\n",
        "\t         [3.06407232,3.005305973,0],\n",
        "\t         [7.627531214,2.759262235,1],\n",
        "\t         [5.332441248,2.088626775,1],\n",
        "\t         [6.922596716,1.77106367,1],\n",
        "\t         [8.675418651,-0.242068655,1],\n",
        "\t         [7.673756466,3.508563011,1]]\n",
        "weights = [-0.1, 0.20653640140000007, -0.23418117710000003] #Knobs that we need to tweak\n",
        "\n",
        "for row in dataset:\n",
        "\tprediction = predict(row, weights)\n",
        "\tprint(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))\n"
      ],
      "metadata": {
        "id": "sghBCZAV0Q97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836763af-2ac5-4b1e-b84b-2407d6a7a13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected=0, Predicted=0\n",
            "Expected=0, Predicted=0\n",
            "Expected=0, Predicted=0\n",
            "Expected=0, Predicted=0\n",
            "Expected=0, Predicted=0\n",
            "Expected=1, Predicted=1\n",
            "Expected=1, Predicted=1\n",
            "Expected=1, Predicted=1\n",
            "Expected=1, Predicted=1\n",
            "Expected=1, Predicted=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The activation equation we have modeled for the above problem is:\n",
        "\n"
      ],
      "metadata": {
        "id": "qF1yhVuizCzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weights must be estimated from your training data using **stochastic gradient descent**.\n",
        "\n",
        "- The way this optimization algorithm works is that each training instance is shown to the model one at a time. \n",
        "- The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction \n",
        "- This procedure can be used to find the set of weights in a model that result in the smallest error for the model on the training data.\n",
        "- For the neuron algorithm, each iteration the weights (w) are updated using the equation: **w = w + learning_rate * (expected - predicted) * x**"
      ],
      "metadata": {
        "id": "jL_AweJVzts3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Optimize Weight Values with Stochastic Gradient Descent\n",
        "\n",
        "Stochastic Gradient Descent requires two parameters, in addition to the training data.\n",
        "- **Learning rate**: Limits the amount each weight is corrected each time it is updated.\n",
        "- **Epochs**: The number of times to run through the training data while updating the weight.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uq1f368K8K_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Make a prediction with weights\n",
        "def predict(row, weights):\n",
        "\tactivation = weights[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tactivation += weights[i + 1] * row[i]\n",
        "\tactivation = 1/(1 + np.exp(-activation))\n",
        "\treturn 1.0 if activation >= 0.5 else 0.0\n",
        "\n",
        "# Estimate the optimal weights using stochastic gradient descent\n",
        "def train_weights(train, l_rate, n_epoch):\n",
        "\tweights = [0.0 for i in range(len(train[0]))]\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tsum_error = 0.0\n",
        "\t\tfor row in train:\n",
        "\t\t\tprediction = predict(row, weights)\n",
        "\t\t\terror = row[-1] - prediction\n",
        "\t\t\tsum_error += error**2\n",
        "\t\t\tweights[0] = weights[0] + l_rate * error\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
        "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        "\treturn weights\n",
        "\n",
        "# Calculate weights\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1]]\n",
        "l_rate = 0.1\n",
        "n_epoch = 5\n",
        "weights = train_weights(dataset, l_rate, n_epoch)\n",
        "print(weights)"
      ],
      "metadata": {
        "id": "4VNwtu6-8feS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63d6c0f-6e74-4544-852f-8691695b7477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">epoch=0, lrate=0.100, error=2.000\n",
            ">epoch=1, lrate=0.100, error=1.000\n",
            ">epoch=2, lrate=0.100, error=0.000\n",
            ">epoch=3, lrate=0.100, error=0.000\n",
            ">epoch=4, lrate=0.100, error=0.000\n",
            "[-0.1, 0.20653640140000007, -0.23418117710000003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Try the neuron algorithm on the Sonar Data Set.\n",
        "Link: sonar data set\n",
        "\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\n"
      ],
      "metadata": {
        "id": "B-JbEhKPWGTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neuron Algorithm on the Sonar Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup\n",
        "\n",
        "# Split a dataset into k folds\n",
        "# Review your 10-fold cross-validation\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n",
        "\n",
        "import numpy as np\n",
        "# Make a prediction with weights\n",
        "def predict(row, weights):\n",
        "\tactivation = weights[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tactivation += weights[i + 1] * row[i]\n",
        "\tactivation = 1/(1 + np.exp(-activation))\n",
        "\treturn 1.0 if activation >= 0.5 else 0.0\n",
        "\n",
        "# Estimate Perceptron weights using stochastic gradient descent\n",
        "def train_weights(train, l_rate, n_epoch):\n",
        "\tweights = [0.0 for i in range(len(train[0]))]\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tfor row in train:\n",
        "\t\t\tprediction = predict(row, weights)\n",
        "\t\t\terror = row[-1] - prediction\n",
        "\t\t\tweights[0] = weights[0] + l_rate * error\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
        "\treturn weights\n",
        "\n",
        "# Perceptron Algorithm With Stochastic Gradient Descent\n",
        "def perceptron(train, test, l_rate, n_epoch):\n",
        "\tpredictions = list()\n",
        "\tweights = train_weights(train, l_rate, n_epoch)\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(row, weights)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)\n",
        "\n",
        "# Test the Perceptron algorithm on the sonar dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'sonar.all-data.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert string class to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# evaluate algorithm\n",
        "n_folds = 3\n",
        "l_rate = 0.01\n",
        "n_epoch = 500\n",
        "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C2wXN8psf9d",
        "outputId": "379c8250-efee-4897-b52d-8ac4517e7635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [76.81159420289855, 69.56521739130434, 72.46376811594203]\n",
            "Mean Accuracy: 72.947%\n"
          ]
        }
      ]
    }
  ]
}